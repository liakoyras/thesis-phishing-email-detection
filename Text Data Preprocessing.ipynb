{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58b3da25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ichanis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ichanis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ichanis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/ichanis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ichanis/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import preprocessing as util\n",
    "from raw_utils import save_to_csv\n",
    "\n",
    "import random\n",
    "random.seed(1746)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb049d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, 'data/csv/')\n",
    "\n",
    "data_files = ['dataset_1.csv', 'dataset_2.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978ee912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3440 entries, 0 to 3439\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   body    3440 non-null   object\n",
      " 1   class   3440 non-null   bool  \n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 57.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_1 = pd.read_csv(os.path.join(csv_path, data_files[0]), index_col=0, encoding='latin-1', dtype={'body': 'object', 'class': 'bool'})\n",
    "dataset_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04b1c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18920 entries, 0 to 18919\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   body    18920 non-null  object\n",
      " 1   class   18920 non-null  bool  \n",
      "dtypes: bool(1), object(1)\n",
      "memory usage: 314.1+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_2 = pd.read_csv(os.path.join(csv_path, data_files[1]), index_col=0, encoding='latin-1', dtype={'body': 'object', 'class': 'bool'})\n",
    "dataset_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fb4dd",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fff1c8",
   "metadata": {},
   "source": [
    "We need to convert the text data into a format more suitable for use with machine learning algorithms.<br>\n",
    "The process will follow the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32dc877",
   "metadata": {},
   "source": [
    "### 1. Cleanup HTML and whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db5573",
   "metadata": {},
   "source": [
    "A percentage of the emails extracted are either in HTML format or they have the same message in both plaintext and HTML, as part of a `multipart/alternative` content-type message.<br>\n",
    "In order to extract the text, the HTML formatting has to be removed, along with unnecessary whitespace any duplicated text created by the aforementioned multipart emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aa0d464",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_1['body'] = dataset_1['body'].apply(util.strip_characters)\n",
    "dataset_1['body'] = dataset_1['body'].apply(util.deduplicate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5905cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2['body'] = dataset_2['body'].apply(util.strip_characters)\n",
    "dataset_2['body'] = dataset_2['body'].apply(util.deduplicate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077db38",
   "metadata": {},
   "source": [
    "### 2. Replacing addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee0a83",
   "metadata": {},
   "source": [
    "A lot of the emails contain either **web addresses** (URLs) or **email addresses** that need to be removed in order for the frequency of certain domains to not influence the results.<br>\n",
    "In order for this information to not get completely lost however, those addresses will be replaced by the strings `<urladdress>` and `<emailaddress>` respectively. Those strings are chosen because they do not occur normally in the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa39079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1['body'] = dataset_1['body'].apply(util.replace_email)\n",
    "dataset_1['body'] = dataset_1['body'].apply(util.replace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcb46d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2['body'] = dataset_2['body'].apply(util.replace_email)\n",
    "dataset_2['body'] = dataset_2['body'].apply(util.replace_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eaa3ce",
   "metadata": {},
   "source": [
    "### 3. Tokenization and stopword removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af826c32",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into individual words. This is useful because generally speaking, the meaning of the text can easily be interpreted by analyzing the words present in the text.<br>\n",
    "Along with this process, letters are also converted to lowercase and punctuation or other special characters are removed.<br>\n",
    "Since there are some words (called **stopwords**) that do not contribute very much in meaning (like pronouns or simple verbs), they can be removed to reduce the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e82c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1['body'] = dataset_1['body'].apply(util.tokenize)\n",
    "dataset_1['body'] = dataset_1['body'].apply(util.remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab3feb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2['body'] = dataset_2['body'].apply(util.tokenize)\n",
    "dataset_2['body'] = dataset_2['body'].apply(util.remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990fbe3",
   "metadata": {},
   "source": [
    "### 4. Lemmatization with POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfef13a",
   "metadata": {},
   "source": [
    "Lemmatization is the process that reduces the inflectional forms of a word to keep its root form. This is useful because the set of words that results from this process is smaller because all the inflections of a word are converted to one, thus reducing the dimensionality without sacrificing information.<br>\n",
    "In order to facilitate and improve the lemmatization, the **part-of-speech tagging** technique has been used. The POS of the word (which indicates whether a word is a noun, a verb, an adjective, or an adverb) is used as a part of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c57d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1['body'] = dataset_1['body'].apply(util.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9614d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2['body'] = dataset_2['body'].apply(util.lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa36d41d",
   "metadata": {},
   "source": [
    "### Deleting Empty Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97905672",
   "metadata": {},
   "source": [
    "After all the preprocessing, it is possible that some of the emails are now empty (because they did not contain any useful words from the beginning).<br>\n",
    "So, these have to be removed to keep the data clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e23a248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = dataset_1[dataset_1['body'].astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eead276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = dataset_2[dataset_2['body'].astype(bool)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3995df5",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b751f",
   "metadata": {},
   "source": [
    "In order to evaluate the classification process, we will use only 80% of the data to train the models and then test them on the remaining 20%, which will be unknown to the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bff283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1 = train_test_split(dataset_1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "782451e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2, test_2 = train_test_split(dataset_2, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acead1",
   "metadata": {},
   "source": [
    "### Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15d3c59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /home/ichanis/projects/phishing_public/data/csv/train_1.csv\n",
      "Saving to /home/ichanis/projects/phishing_public/data/csv/test_1.csv\n"
     ]
    }
   ],
   "source": [
    "save_to_csv(train_1, csv_path, 'train_1.csv')\n",
    "save_to_csv(test_1, csv_path, 'test_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e564015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /home/ichanis/projects/phishing_public/data/csv/train_2.csv\n",
      "Saving to /home/ichanis/projects/phishing_public/data/csv/test_2.csv\n"
     ]
    }
   ],
   "source": [
    "save_to_csv(train_2, csv_path, 'train_2.csv')\n",
    "save_to_csv(test_2, csv_path, 'test_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

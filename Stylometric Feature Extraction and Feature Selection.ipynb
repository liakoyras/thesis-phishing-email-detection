{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1802f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 160\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import features as util\n",
    "from features import language_tool_python\n",
    "from preprocessing import tokenize\n",
    "from raw_utils import save_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f5f8",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af55909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, 'data/csv/')\n",
    "\n",
    "train_text = ['train_balanced_text.csv', 'train_imbalanced_text.csv']\n",
    "test_text = ['test_balanced_text.csv', 'test_imbalanced_text.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac3b15",
   "metadata": {},
   "source": [
    "#### Email Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48a6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_text = pd.read_csv(os.path.join(csv_path, train_text[0]), index_col=0)\n",
    "test_balanced_text = pd.read_csv(os.path.join(csv_path, test_text[0]), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e93526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_text = pd.read_csv(os.path.join(csv_path, train_text[1]), index_col=0)\n",
    "test_imbalanced_text = pd.read_csv(os.path.join(csv_path, test_text[1]), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27680c4a",
   "metadata": {},
   "source": [
    "After the preprocessing, the data look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f271a3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2377</td>\n",
       "      <td>please print\\n----- Forwarded by Steven J Kean/NA/Enron on 10/16/2000 10:27 AM -----\\n\\n\\tCynthia Sandherr\\n\\t10/12/2000 07:43 PM\\n\\t\\t \\n\\t\\t To: Thomas E ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>592</td>\n",
       "      <td>Server Message \\n  \\n\\n \\n   Dear &lt;emailaddress&gt;  Our record indicates that you recently made a request to deactivate email And this request will be process...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1049</td>\n",
       "      <td>Please see the attached articles:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1087</td>\n",
       "      <td>Please Click Here&lt;urladdress&gt; to Update e-mail Password\\n\\n\\n\\nIT Security immediately/\\n\\n________________________________\\nSEED IS PROUD TO BE A 21st CENT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725</td>\n",
       "      <td>Wednesday afternoon Febuary 6th, 2002, the Enron building experienced a brief power outage.  The building is powered by one of two Reliant circuits.  Yester...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  2377   \n",
       "1   592   \n",
       "2  1049   \n",
       "3  1087   \n",
       "4   725   \n",
       "\n",
       "                                                                                                                                                              body  \\\n",
       "0  please print\\n----- Forwarded by Steven J Kean/NA/Enron on 10/16/2000 10:27 AM -----\\n\\n\\tCynthia Sandherr\\n\\t10/12/2000 07:43 PM\\n\\t\\t \\n\\t\\t To: Thomas E ...   \n",
       "1  Server Message \\n  \\n\\n \\n   Dear <emailaddress>  Our record indicates that you recently made a request to deactivate email And this request will be process...   \n",
       "2                                                                                                                                Please see the attached articles:   \n",
       "3  Please Click Here<urladdress> to Update e-mail Password\\n\\n\\n\\nIT Security immediately/\\n\\n________________________________\\nSEED IS PROUD TO BE A 21st CENT...   \n",
       "4  Wednesday afternoon Febuary 6th, 2002, the Enron building experienced a brief power outage.  The building is powered by one of two Reliant circuits.  Yester...   \n",
       "\n",
       "   class  \n",
       "0  False  \n",
       "1   True  \n",
       "2  False  \n",
       "3   True  \n",
       "4  False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948acf1c",
   "metadata": {},
   "source": [
    "#### Email Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28707c",
   "metadata": {},
   "source": [
    "Since the .csv files with the already tokenized emails have been subject to further preprocessing like lemmatization and stopword removal, a simple tokenization (at word and sentence level) will also be run here for the purposes of feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c970d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_text['tokens'] = train_balanced_text['body'].apply(tokenize)\n",
    "test_balanced_text['tokens'] = test_balanced_text['body'].apply(tokenize)\n",
    "train_imbalanced_text['tokens'] = train_imbalanced_text['body'].apply(tokenize)\n",
    "test_imbalanced_text['tokens'] = test_imbalanced_text['body'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e357899",
   "metadata": {},
   "source": [
    "Note that for the sentence-level tokenization, `nltk.sent_tokenization` is used, so any sentences separated by a newline without punctuation will be considered simply as wrapped text, and not new, different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a3ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_text['sentences'] = train_balanced_text['body'].apply(sent_tokenize)\n",
    "test_balanced_text['sentences'] = test_balanced_text['body'].apply(sent_tokenize)\n",
    "train_imbalanced_text['sentences'] = train_imbalanced_text['body'].apply(sent_tokenize)\n",
    "test_imbalanced_text['sentences'] = test_imbalanced_text['body'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdcf9b",
   "metadata": {},
   "source": [
    "# Stylometric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f9c06",
   "metadata": {},
   "source": [
    "Useful markers of whether an email is phishing or not should stem from the writing style of the author.<br>\n",
    "With this in mind, several features that have been previously used in the literature will be extracted, in order to be compared and combined with the text-only baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe758756",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style = train_balanced_text[['id', 'class']].copy()\n",
    "test_balanced_style = test_balanced_text[['id', 'class']].copy()\n",
    "train_imbalanced_style = train_imbalanced_text[['id', 'class']].copy()\n",
    "test_imbalanced_style = test_imbalanced_text[['id', 'class']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6019e",
   "metadata": {},
   "source": [
    "### Simple Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbfcb7",
   "metadata": {},
   "source": [
    "The simplest kind of features would be simple counts of various parts of the emails, like characters and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1f771bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style['num_chars'] = train_balanced_text['body'].apply(util.count_chars)\n",
    "train_balanced_style['num_newlines'] = train_balanced_text['body'].apply(util.count_newlines)\n",
    "train_balanced_style['num_special_chars'] = train_balanced_text['body'].apply(util.count_special_chars)\n",
    "train_balanced_style['num_words'] = train_balanced_text['tokens'].apply(util.count_words)\n",
    "train_balanced_style['num_unique_words'] = train_balanced_text['tokens'].apply(util.count_unique_words)\n",
    "train_balanced_style['sentences'] = train_balanced_text['sentences'].apply(util.count_sentences)\n",
    "train_balanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(train_balanced_style['sentences'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b11e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style['num_chars'] = test_balanced_text['body'].apply(util.count_chars)\n",
    "test_balanced_style['num_newlines'] = test_balanced_text['body'].apply(util.count_newlines)\n",
    "test_balanced_style['num_special_chars'] = test_balanced_text['body'].apply(util.count_special_chars)\n",
    "test_balanced_style['num_words'] = test_balanced_text['tokens'].apply(util.count_words)\n",
    "test_balanced_style['num_unique_words'] = test_balanced_text['tokens'].apply(util.count_unique_words)\n",
    "test_balanced_style['sentences'] = test_balanced_text['sentences'].apply(util.count_sentences)\n",
    "test_balanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(test_balanced_style['sentences'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058d0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_style['num_chars'] = train_imbalanced_text['body'].apply(util.count_chars)\n",
    "train_imbalanced_style['num_newlines'] = train_imbalanced_text['body'].apply(util.count_newlines)\n",
    "train_imbalanced_style['num_special_chars'] = train_imbalanced_text['body'].apply(util.count_special_chars)\n",
    "train_imbalanced_style['num_words'] = train_imbalanced_text['tokens'].apply(util.count_words)\n",
    "train_imbalanced_style['num_unique_words'] = train_imbalanced_text['tokens'].apply(util.count_unique_words)\n",
    "train_imbalanced_style['sentences'] = train_imbalanced_text['sentences'].apply(util.count_sentences)\n",
    "train_imbalanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(train_imbalanced_style['sentences'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88450b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imbalanced_style['num_chars'] = test_imbalanced_text['body'].apply(util.count_chars)\n",
    "test_imbalanced_style['num_newlines'] = test_imbalanced_text['body'].apply(util.count_newlines)\n",
    "test_imbalanced_style['num_special_chars'] = test_imbalanced_text['body'].apply(util.count_special_chars)\n",
    "test_imbalanced_style['num_words'] = test_imbalanced_text['tokens'].apply(util.count_words)\n",
    "test_imbalanced_style['num_unique_words'] = test_imbalanced_text['tokens'].apply(util.count_unique_words)\n",
    "test_imbalanced_style['sentences'] = test_imbalanced_text['sentences'].apply(util.count_sentences)\n",
    "test_imbalanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(test_imbalanced_style['sentences'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefcdc73",
   "metadata": {},
   "source": [
    "## Word Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d89241",
   "metadata": {},
   "source": [
    "Another category of features are those that related to the size of words, like the average size and counts or frequencies of smaller or bigger words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9586db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style['avg_word_size'] = train_balanced_text['tokens'].apply(util.average_word_length)\n",
    "train_balanced_style['small_words'] = train_balanced_text['tokens'].apply(util.small_words)\n",
    "train_balanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(train_balanced_style['small_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['big_words'] = train_balanced_text['tokens'].apply(util.big_words)\n",
    "train_balanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(train_balanced_style['big_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['huge_words'] = train_balanced_text['tokens'].apply(util.huge_words)\n",
    "train_balanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(train_balanced_style['huge_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bbded68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style['avg_word_size'] = test_balanced_text['tokens'].apply(util.average_word_length)\n",
    "test_balanced_style['small_words'] = test_balanced_text['tokens'].apply(util.small_words)\n",
    "test_balanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(test_balanced_style['small_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['big_words'] = test_balanced_text['tokens'].apply(util.big_words)\n",
    "test_balanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(test_balanced_style['big_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['huge_words'] = test_balanced_text['tokens'].apply(util.huge_words)\n",
    "test_balanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(test_balanced_style['huge_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54525efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_style['avg_word_size'] = train_imbalanced_text['tokens'].apply(util.average_word_length)\n",
    "train_imbalanced_style['small_words'] = train_imbalanced_text['tokens'].apply(util.small_words)\n",
    "train_imbalanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(train_imbalanced_style['small_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['big_words'] = train_imbalanced_text['tokens'].apply(util.big_words)\n",
    "train_imbalanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(train_imbalanced_style['big_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['huge_words'] = train_imbalanced_text['tokens'].apply(util.huge_words)\n",
    "train_imbalanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(train_imbalanced_style['huge_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44b712d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imbalanced_style['avg_word_size'] = test_imbalanced_text['tokens'].apply(util.average_word_length)\n",
    "test_imbalanced_style['small_words'] = test_imbalanced_text['tokens'].apply(util.small_words)\n",
    "test_imbalanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(test_imbalanced_style['small_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['big_words'] = test_imbalanced_text['tokens'].apply(util.big_words)\n",
    "test_imbalanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(test_imbalanced_style['big_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['huge_words'] = test_imbalanced_text['tokens'].apply(util.huge_words)\n",
    "test_imbalanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(test_imbalanced_style['huge_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a91765",
   "metadata": {},
   "source": [
    "## Sentence Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d0aedb",
   "metadata": {},
   "source": [
    "Another set of features could be related to various simple statistics about the size of a sentence, using both characters and words as a unit of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02da8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style['avg_sent_size'] = train_balanced_text['sentences'].apply(util.average_sentence_length)\n",
    "train_balanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(train_balanced_style['avg_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['std_sent_size'] = train_balanced_text['sentences'].apply(util.std_sentence_length)\n",
    "train_balanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(train_balanced_style['std_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['min_sent_size'] = train_balanced_text['sentences'].apply(util.min_sentence_length)\n",
    "train_balanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(train_balanced_style['min_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['max_sent_size'] = train_balanced_text['sentences'].apply(util.max_sentence_length)\n",
    "train_balanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(train_balanced_style['max_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "073706f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style['avg_sent_size'] = test_balanced_text['sentences'].apply(util.average_sentence_length)\n",
    "test_balanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(test_balanced_style['avg_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['std_sent_size'] = test_balanced_text['sentences'].apply(util.std_sentence_length)\n",
    "test_balanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(test_balanced_style['std_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['min_sent_size'] = test_balanced_text['sentences'].apply(util.min_sentence_length)\n",
    "test_balanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(test_balanced_style['min_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['max_sent_size'] = test_balanced_text['sentences'].apply(util.max_sentence_length)\n",
    "test_balanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(test_balanced_style['max_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f66ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_style['avg_sent_size'] = train_imbalanced_text['sentences'].apply(util.average_sentence_length)\n",
    "train_imbalanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(train_imbalanced_style['avg_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['std_sent_size'] = train_imbalanced_text['sentences'].apply(util.std_sentence_length)\n",
    "train_imbalanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(train_imbalanced_style['std_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['min_sent_size'] = train_imbalanced_text['sentences'].apply(util.min_sentence_length)\n",
    "train_imbalanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(train_imbalanced_style['min_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['max_sent_size'] = train_imbalanced_text['sentences'].apply(util.max_sentence_length)\n",
    "train_imbalanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(train_imbalanced_style['max_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20e5e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imbalanced_style['avg_sent_size'] = test_imbalanced_text['sentences'].apply(util.average_sentence_length)\n",
    "test_imbalanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(test_imbalanced_style['avg_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['std_sent_size'] = test_imbalanced_text['sentences'].apply(util.std_sentence_length)\n",
    "test_imbalanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(test_imbalanced_style['std_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['min_sent_size'] = test_imbalanced_text['sentences'].apply(util.min_sentence_length)\n",
    "test_imbalanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(test_imbalanced_style['min_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['max_sent_size'] = test_imbalanced_text['sentences'].apply(util.max_sentence_length)\n",
    "test_imbalanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(test_imbalanced_style['max_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0f7d4",
   "metadata": {},
   "source": [
    "## Ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4baee",
   "metadata": {},
   "source": [
    "There are also the ratios of various text elements (like specific characters to total characters or words to characters) that can be used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df606b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style['words_to_chars'] = util.series_ratio(train_balanced_style['num_words'], train_balanced_style['num_chars'])\n",
    "train_balanced_style['unique_words_to_words'] = util.series_ratio(train_balanced_style['num_unique_words'], train_balanced_style['num_words'])\n",
    "train_balanced_style['special_chars_to_chars'] = util.series_ratio(train_balanced_style['num_special_chars'], train_balanced_style['num_chars'])\n",
    "\n",
    "train_balanced_style['dots_to_chars'] = train_balanced_text['body'].apply(util.character_to_chars, character='.')\n",
    "train_balanced_style['commas_to_chars'] = train_balanced_text['body'].apply(util.character_to_chars, character=',')\n",
    "train_balanced_style['questionmark_to_chars'] = train_balanced_text['body'].apply(util.character_to_chars, character='?')\n",
    "train_balanced_style['exclamationmark_to_chars'] = train_balanced_text['body'].apply(util.character_to_chars, character='!')\n",
    "\n",
    "train_balanced_style['chars_to_lines'] = train_balanced_text['body'].apply(util.chars_to_lines)\n",
    "train_balanced_style['alpha_tokens_to_words'] = train_balanced_text['body'].apply(util.alpha_tokens_ratio)\n",
    "\n",
    "train_balanced_style['words_to_lines'] = train_balanced_style['words_to_chars'] * train_balanced_style['chars_to_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5c11d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style['words_to_chars'] = util.series_ratio(test_balanced_style['num_words'], test_balanced_style['num_chars'])\n",
    "test_balanced_style['unique_words_to_words'] = util.series_ratio(test_balanced_style['num_unique_words'], test_balanced_style['num_words'])\n",
    "test_balanced_style['special_chars_to_chars'] = util.series_ratio(test_balanced_style['num_special_chars'], test_balanced_style['num_chars'])\n",
    "\n",
    "test_balanced_style['dots_to_chars'] = test_balanced_text['body'].apply(util.character_to_chars, character='.')\n",
    "test_balanced_style['commas_to_chars'] = test_balanced_text['body'].apply(util.character_to_chars, character=',')\n",
    "test_balanced_style['questionmark_to_chars'] = test_balanced_text['body'].apply(util.character_to_chars, character='?')\n",
    "test_balanced_style['exclamationmark_to_chars'] = test_balanced_text['body'].apply(util.character_to_chars, character='!')\n",
    "\n",
    "test_balanced_style['chars_to_lines'] = test_balanced_text['body'].apply(util.chars_to_lines)\n",
    "test_balanced_style['alpha_tokens_to_words'] = test_balanced_text['body'].apply(util.alpha_tokens_ratio)\n",
    "\n",
    "test_balanced_style['words_to_lines'] = test_balanced_style['words_to_chars'] * test_balanced_style['chars_to_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29cce630",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_style['words_to_chars'] = util.series_ratio(train_imbalanced_style['num_words'], train_imbalanced_style['num_chars'])\n",
    "train_imbalanced_style['unique_words_to_words'] = util.series_ratio(train_imbalanced_style['num_unique_words'], train_imbalanced_style['num_words'])\n",
    "train_imbalanced_style['special_chars_to_chars'] = util.series_ratio(train_imbalanced_style['num_special_chars'], train_imbalanced_style['num_chars'])\n",
    "\n",
    "train_imbalanced_style['dots_to_chars'] = train_imbalanced_text['body'].apply(util.character_to_chars, character='.')\n",
    "train_imbalanced_style['commas_to_chars'] = train_imbalanced_text['body'].apply(util.character_to_chars, character=',')\n",
    "train_imbalanced_style['questionmark_to_chars'] = train_imbalanced_text['body'].apply(util.character_to_chars, character='?')\n",
    "train_imbalanced_style['exclamationmark_to_chars'] = train_imbalanced_text['body'].apply(util.character_to_chars, character='!')\n",
    "\n",
    "train_imbalanced_style['chars_to_lines'] = train_imbalanced_text['body'].apply(util.chars_to_lines)\n",
    "train_imbalanced_style['alpha_tokens_to_words'] = train_imbalanced_text['body'].apply(util.alpha_tokens_ratio)\n",
    "\n",
    "train_imbalanced_style['words_to_lines'] = train_imbalanced_style['words_to_chars'] * train_imbalanced_style['chars_to_lines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac7736d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imbalanced_style['words_to_chars'] = util.series_ratio(test_imbalanced_style['num_words'], test_imbalanced_style['num_chars'])\n",
    "test_imbalanced_style['unique_words_to_words'] = util.series_ratio(test_imbalanced_style['num_unique_words'], test_imbalanced_style['num_words'])\n",
    "test_imbalanced_style['special_chars_to_chars'] = util.series_ratio(test_imbalanced_style['num_special_chars'], test_imbalanced_style['num_chars'])\n",
    "\n",
    "test_imbalanced_style['dots_to_chars'] = test_imbalanced_text['body'].apply(util.character_to_chars, character='.')\n",
    "test_imbalanced_style['commas_to_chars'] = test_imbalanced_text['body'].apply(util.character_to_chars, character=',')\n",
    "test_imbalanced_style['questionmark_to_chars'] = test_imbalanced_text['body'].apply(util.character_to_chars, character='?')\n",
    "test_imbalanced_style['exclamationmark_to_chars'] = test_imbalanced_text['body'].apply(util.character_to_chars, character='!')\n",
    "\n",
    "test_imbalanced_style['chars_to_lines'] = test_imbalanced_text['body'].apply(util.chars_to_lines)\n",
    "test_imbalanced_style['alpha_tokens_to_words'] = test_imbalanced_text['body'].apply(util.alpha_tokens_ratio)\n",
    "\n",
    "test_imbalanced_style['words_to_lines'] = test_imbalanced_style['words_to_chars'] * test_imbalanced_style['chars_to_lines']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5e9a8",
   "metadata": {},
   "source": [
    "## Readability and Spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec2015",
   "metadata": {},
   "source": [
    "Finally, somewhat more advanced readability scores and spelling/grammatical errors can be used as features.<br>\n",
    "Note that the spellcheck is a time consuming procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9f12d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d5798f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_tool = language_tool_python.LanguageTool('en-US', config={'cacheSize': 1000, 'pipelineCaching': True, 'maxCheckThreads': 12, 'maxSpellingSuggestions': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8c6a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for balanced train set spellcheck:  837.70  seconds.\n"
     ]
    }
   ],
   "source": [
    "train_balanced_style['readability'] = train_balanced_text['body'].apply(util.readability)\n",
    "train_balanced_style[['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau', 'dale_chall', 'autmated_readability_index', 'linsear_write', 'spache']] = pd.DataFrame(train_balanced_style['readability'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop('readability', axis=1)\n",
    "\n",
    "t1 = time()\n",
    "train_balanced_style['errors'] = train_balanced_text['body'].apply(util.errors_check, tool=language_tool)\n",
    "t2 = time()\n",
    "print(\"Time required for balanced train set spellcheck: \", \"{:.2f}\".format(t2-t1), \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "828cce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for balanced test set spellcheck:  61.34  seconds.\n"
     ]
    }
   ],
   "source": [
    "test_balanced_style['readability'] = test_balanced_text['body'].apply(util.readability)\n",
    "test_balanced_style[['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau', 'dale_chall', 'autmated_readability_index', 'linsear_write', 'spache']] = pd.DataFrame(test_balanced_style['readability'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop('readability', axis=1)\n",
    "\n",
    "t1 = time()\n",
    "test_balanced_style['errors'] = test_balanced_text['body'].apply(util.errors_check, tool=language_tool)\n",
    "t2 = time()\n",
    "print(\"Time required for balanced test set spellcheck: \", \"{:.2f}\".format(t2-t1), \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9bcc44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for imbalanced train set spellcheck:  2193.11  seconds.\n"
     ]
    }
   ],
   "source": [
    "train_imbalanced_style['readability'] = train_imbalanced_text['body'].apply(util.readability)\n",
    "train_imbalanced_style[['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau', 'dale_chall', 'autmated_readability_index', 'linsear_write', 'spache']] = pd.DataFrame(train_imbalanced_style['readability'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop('readability', axis=1)\n",
    "\n",
    "t1 = time()\n",
    "train_imbalanced_style['errors'] = train_imbalanced_text['body'].apply(util.errors_check, tool=language_tool)\n",
    "t2 = time()\n",
    "print(\"Time required for imbalanced train set spellcheck: \", \"{:.2f}\".format(t2-t1), \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acfe434d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time required for imbalanced test set spellcheck:  391.66  seconds.\n"
     ]
    }
   ],
   "source": [
    "test_imbalanced_style['readability'] = test_imbalanced_text['body'].apply(util.readability)\n",
    "test_imbalanced_style[['flesch_kincaid', 'flesch', 'gunning_fog', 'coleman_liau', 'dale_chall', 'autmated_readability_index', 'linsear_write', 'spache']] = pd.DataFrame(test_imbalanced_style['readability'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop('readability', axis=1)\n",
    "\n",
    "t1 = time()\n",
    "test_imbalanced_style['errors'] = test_imbalanced_text['body'].apply(util.errors_check, tool=language_tool)\n",
    "t2 = time()\n",
    "print(\"Time required for imbalanced test set spellcheck: \", \"{:.2f}\".format(t2-t1), \" seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f945a90",
   "metadata": {},
   "source": [
    "# Final Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1c0c14",
   "metadata": {},
   "source": [
    "The calculated features look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4426907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_newlines</th>\n",
       "      <th>num_special_chars</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_upper_sentences</th>\n",
       "      <th>num_lower_sentences</th>\n",
       "      <th>...</th>\n",
       "      <th>words_to_lines</th>\n",
       "      <th>flesch_kincaid</th>\n",
       "      <th>flesch</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>coleman_liau</th>\n",
       "      <th>dale_chall</th>\n",
       "      <th>autmated_readability_index</th>\n",
       "      <th>linsear_write</th>\n",
       "      <th>spache</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2377</td>\n",
       "      <td>False</td>\n",
       "      <td>1884</td>\n",
       "      <td>36</td>\n",
       "      <td>98</td>\n",
       "      <td>264</td>\n",
       "      <td>161</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>7.135135</td>\n",
       "      <td>13.682413</td>\n",
       "      <td>45.451678</td>\n",
       "      <td>15.960839</td>\n",
       "      <td>11.237179</td>\n",
       "      <td>11.723721</td>\n",
       "      <td>15.245087</td>\n",
       "      <td>18.363636</td>\n",
       "      <td>9.386350</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>592</td>\n",
       "      <td>True</td>\n",
       "      <td>1311</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "      <td>195</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.062500</td>\n",
       "      <td>13.520256</td>\n",
       "      <td>35.166410</td>\n",
       "      <td>13.384615</td>\n",
       "      <td>13.530462</td>\n",
       "      <td>10.784244</td>\n",
       "      <td>13.991949</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>7.554513</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1049</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1087</td>\n",
       "      <td>True</td>\n",
       "      <td>1636</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>242</td>\n",
       "      <td>82</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.736842</td>\n",
       "      <td>15.826939</td>\n",
       "      <td>23.471990</td>\n",
       "      <td>19.269388</td>\n",
       "      <td>14.887837</td>\n",
       "      <td>10.716557</td>\n",
       "      <td>16.369347</td>\n",
       "      <td>18.850000</td>\n",
       "      <td>8.260031</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725</td>\n",
       "      <td>False</td>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>115</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>12.405901</td>\n",
       "      <td>34.201739</td>\n",
       "      <td>13.527950</td>\n",
       "      <td>15.530783</td>\n",
       "      <td>10.630053</td>\n",
       "      <td>13.324112</td>\n",
       "      <td>11.785714</td>\n",
       "      <td>7.418037</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  class  num_chars  num_newlines  num_special_chars  num_words  \\\n",
       "0  2377  False       1884            36                 98        264   \n",
       "1   592   True       1311            47                 36        195   \n",
       "2  1049  False         33             0                  1          5   \n",
       "3  1087   True       1636            18                 66        242   \n",
       "4   725  False        784             2                 16        115   \n",
       "\n",
       "   num_unique_words  num_sentences  num_upper_sentences  num_lower_sentences  \\\n",
       "0               161             11                    9                    2   \n",
       "1                60              9                    9                    0   \n",
       "2                 5              1                    1                    0   \n",
       "3                82             10                   10                    0   \n",
       "4                81              7                    7                    0   \n",
       "\n",
       "   ...  words_to_lines  flesch_kincaid     flesch  gunning_fog  coleman_liau  \\\n",
       "0  ...        7.135135       13.682413  45.451678    15.960839     11.237179   \n",
       "1  ...        4.062500       13.520256  35.166410    13.384615     13.530462   \n",
       "2  ...        5.000000             NaN        NaN          NaN           NaN   \n",
       "3  ...       12.736842       15.826939  23.471990    19.269388     14.887837   \n",
       "4  ...       38.333333       12.405901  34.201739    13.527950     15.530783   \n",
       "\n",
       "   dale_chall  autmated_readability_index  linsear_write    spache  errors  \n",
       "0   11.723721                   15.245087      18.363636  9.386350      22  \n",
       "1   10.784244                   13.991949      14.500000  7.554513      17  \n",
       "2         NaN                         NaN            NaN       NaN       0  \n",
       "3   10.716557                   16.369347      18.850000  8.260031       1  \n",
       "4   10.630053                   13.324112      11.785714  7.418037       2  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced_style.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2aadec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(674, 44)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced_style.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91196e96",
   "metadata": {},
   "source": [
    "In total, 44 features where created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9f330",
   "metadata": {},
   "source": [
    "For consistency, change the id and class column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e1f57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style = train_balanced_style.rename(columns={\"id\": \"email_id\", \"class\": \"email_class\"})\n",
    "test_balanced_style = test_balanced_style.rename(columns={\"id\": \"email_id\", \"class\": \"email_class\"})\n",
    "\n",
    "train_imbalanced_style = train_imbalanced_style.rename(columns={\"id\": \"email_id\", \"class\": \"email_class\"})\n",
    "test_imbalanced_style = test_imbalanced_style.rename(columns={\"id\": \"email_id\", \"class\": \"email_class\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f8a74",
   "metadata": {},
   "source": [
    "## Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5035f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /home/ichanis/projects/phishing_public/data/csv/style_train_balanced.csv\n",
      "Saving to /home/ichanis/projects/phishing_public/data/csv/style_test_balanced.csv\n",
      "Saving to /home/ichanis/projects/phishing_public/data/csv/style_train_imbalanced.csv\n",
      "Saving to /home/ichanis/projects/phishing_public/data/csv/style_test_imbalanced.csv\n"
     ]
    }
   ],
   "source": [
    "save_to_csv(train_balanced_style, csv_path, 'style_train_balanced.csv')\n",
    "save_to_csv(test_balanced_style, csv_path, 'style_test_balanced.csv')\n",
    "\n",
    "save_to_csv(train_imbalanced_style, csv_path, 'style_train_imbalanced.csv')\n",
    "save_to_csv(test_imbalanced_style, csv_path, 'style_test_imbalanced.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

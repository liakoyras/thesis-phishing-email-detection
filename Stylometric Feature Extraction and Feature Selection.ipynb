{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1802f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_colwidth = 160\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import features as util\n",
    "from raw_utils import save_to_csv\n",
    "from preprocessing import tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f5f8",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af55909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "cwd = os.getcwd()\n",
    "csv_path = os.path.join(cwd, 'data/csv/')\n",
    "\n",
    "train_text = ['train_balanced_text.csv', 'train_imbalanced_text.csv']\n",
    "test_text = ['test_balanced_text.csv', 'test_imbalanced_text.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac3b15",
   "metadata": {},
   "source": [
    "#### Email Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48a6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_text = pd.read_csv(os.path.join(csv_path, train_text[0]), index_col=0)\n",
    "test_balanced_text = pd.read_csv(os.path.join(csv_path, test_text[0]), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e93526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_text = pd.read_csv(os.path.join(csv_path, train_text[1]), index_col=0)\n",
    "test_imbalanced_text = pd.read_csv(os.path.join(csv_path, test_text[1]), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27680c4a",
   "metadata": {},
   "source": [
    "After the preprocessing, the data look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f271a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2377</td>\n",
       "      <td>please print\\n----- Forwarded by Steven J Kean/NA/Enron on 10/16/2000 10:27 AM -----\\n\\n\\tCynthia Sandherr\\n\\t10/12/2000 07:43 PM\\n\\t\\t \\n\\t\\t To: Thomas E ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>592</td>\n",
       "      <td>Server Message \\n  \\n\\n \\n   Dear &lt;emailaddress&gt;  Our record indicates that you recently made a request to deactivate email And this request will be process...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1049</td>\n",
       "      <td>Please see the attached articles:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1087</td>\n",
       "      <td>Please Click Here&lt;urladdress&gt; to Update e-mail Password\\n\\n\\n\\nIT Security immediately/\\n\\n________________________________\\nSEED IS PROUD TO BE A 21st CENT...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725</td>\n",
       "      <td>Wednesday afternoon Febuary 6th, 2002, the Enron building experienced a brief power outage.  The building is powered by one of two Reliant circuits.  Yester...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  2377   \n",
       "1   592   \n",
       "2  1049   \n",
       "3  1087   \n",
       "4   725   \n",
       "\n",
       "                                                                                                                                                              body  \\\n",
       "0  please print\\n----- Forwarded by Steven J Kean/NA/Enron on 10/16/2000 10:27 AM -----\\n\\n\\tCynthia Sandherr\\n\\t10/12/2000 07:43 PM\\n\\t\\t \\n\\t\\t To: Thomas E ...   \n",
       "1  Server Message \\n  \\n\\n \\n   Dear <emailaddress>  Our record indicates that you recently made a request to deactivate email And this request will be process...   \n",
       "2                                                                                                                                Please see the attached articles:   \n",
       "3  Please Click Here<urladdress> to Update e-mail Password\\n\\n\\n\\nIT Security immediately/\\n\\n________________________________\\nSEED IS PROUD TO BE A 21st CENT...   \n",
       "4  Wednesday afternoon Febuary 6th, 2002, the Enron building experienced a brief power outage.  The building is powered by one of two Reliant circuits.  Yester...   \n",
       "\n",
       "   class  \n",
       "0  False  \n",
       "1   True  \n",
       "2  False  \n",
       "3   True  \n",
       "4  False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d730451",
   "metadata": {},
   "source": [
    "#### Email Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2273bd03",
   "metadata": {},
   "source": [
    "Since the .csv files with the already tokenized emails have been subject to further preprocessing like lemmatization and stopword removal, a simple tokenization (at word and sentence level) will also be run here for the purposes of feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09373ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_text['tokens'] = train_balanced_text['body'].apply(tokenize)\n",
    "test_balanced_text['tokens'] = test_balanced_text['body'].apply(tokenize)\n",
    "train_imbalanced_text['tokens'] = train_imbalanced_text['body'].apply(tokenize)\n",
    "test_imbalanced_text['tokens'] = test_imbalanced_text['body'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7e9f29",
   "metadata": {},
   "source": [
    "Note that for the sentence-level tokenization, `nltk.sent_tokenization` is used, so any sentences separated by a newline without punctuation will be considered simply as wrapped text, and not new, different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e555bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_text['sentences'] = train_balanced_text['body'].apply(sent_tokenize)\n",
    "test_balanced_text['sentences'] = test_balanced_text['body'].apply(sent_tokenize)\n",
    "train_imbalanced_text['sentences'] = train_imbalanced_text['body'].apply(sent_tokenize)\n",
    "test_imbalanced_text['sentences'] = test_imbalanced_text['body'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdcf9b",
   "metadata": {},
   "source": [
    "# Stylometric Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f9c06",
   "metadata": {},
   "source": [
    "A useful marker of wether an email is phishing or not should stem from the writing style of the author.<br>\n",
    "With this in mind, several features that have been used in the literature will be extracted, in order to be compared and combined with the text-only baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe758756",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style = train_balanced_text[['id', 'class']].copy()\n",
    "test_balanced_style = test_balanced_text[['id', 'class']].copy()\n",
    "train_imbalanced_style = train_imbalanced_text[['id', 'class']].copy()\n",
    "test_imbalanced_style = test_imbalanced_text[['id', 'class']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6019e",
   "metadata": {},
   "source": [
    "### Simple Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bbfcb7",
   "metadata": {},
   "source": [
    "The simplest kind of features would be simple counts of various parts of the emails, like characters and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2cd23f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style['num_chars'] = train_balanced_text['body'].apply(util.count_chars)\n",
    "train_balanced_style['num_newlines'] = train_balanced_text['body'].apply(util.count_newlines)\n",
    "train_balanced_style['num_words'] = train_balanced_text['tokens'].apply(util.count_words)\n",
    "train_balanced_style['num_unique_words'] = train_balanced_text['tokens'].apply(util.count_unique_words)\n",
    "train_balanced_style['sentences'] = train_balanced_text['sentences'].apply(util.count_sentences)\n",
    "train_balanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(train_balanced_style['sentences'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b11e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style['num_chars'] = test_balanced_text['body'].apply(util.count_chars)\n",
    "test_balanced_style['num_newlines'] = test_balanced_text['body'].apply(util.count_newlines)\n",
    "test_balanced_style['num_words'] = test_balanced_text['tokens'].apply(util.count_words)\n",
    "test_balanced_style['num_unique_words'] = test_balanced_text['tokens'].apply(util.count_unique_words)\n",
    "test_balanced_style['sentences'] = test_balanced_text['sentences'].apply(util.count_sentences)\n",
    "test_balanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(test_balanced_style['sentences'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "898791ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_style['num_chars'] = train_imbalanced_text['body'].apply(util.count_chars)\n",
    "train_imbalanced_style['num_newlines'] = train_imbalanced_text['body'].apply(util.count_newlines)\n",
    "train_imbalanced_style['num_words'] = train_imbalanced_text['tokens'].apply(util.count_words)\n",
    "train_imbalanced_style['num_unique_words'] = train_imbalanced_text['tokens'].apply(util.count_unique_words)\n",
    "train_imbalanced_style['sentences'] = train_imbalanced_text['sentences'].apply(util.count_sentences)\n",
    "train_imbalanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(train_imbalanced_style['sentences'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c4d61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imbalanced_style['num_chars'] = test_imbalanced_text['body'].apply(util.count_chars)\n",
    "test_imbalanced_style['num_newlines'] = test_imbalanced_text['body'].apply(util.count_newlines)\n",
    "test_imbalanced_style['num_words'] = test_imbalanced_text['tokens'].apply(util.count_words)\n",
    "test_imbalanced_style['num_unique_words'] = test_imbalanced_text['tokens'].apply(util.count_unique_words)\n",
    "test_imbalanced_style['sentences'] = test_imbalanced_text['sentences'].apply(util.count_sentences)\n",
    "test_imbalanced_style[['num_sentences', 'num_upper_sentences', 'num_lower_sentences']] = pd.DataFrame(test_imbalanced_style['sentences'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop('sentences', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1843487",
   "metadata": {},
   "source": [
    "## Word Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec560d",
   "metadata": {},
   "source": [
    "Another category of features are those that related to the size of words, like the average size and counts or frequencies of smaller or bigger words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee67f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style['avg_word_size'] = train_balanced_text['tokens'].apply(util.average_word_length)\n",
    "train_balanced_style['small_words'] = train_balanced_text['tokens'].apply(util.small_words)\n",
    "train_balanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(train_balanced_style['small_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['big_words'] = train_balanced_text['tokens'].apply(util.big_words)\n",
    "train_balanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(train_balanced_style['big_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['huge_words'] = train_balanced_text['tokens'].apply(util.huge_words)\n",
    "train_balanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(train_balanced_style['huge_words'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6433d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style['avg_word_size'] = test_balanced_text['tokens'].apply(util.average_word_length)\n",
    "test_balanced_style['small_words'] = test_balanced_text['tokens'].apply(util.small_words)\n",
    "test_balanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(test_balanced_style['small_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['big_words'] = test_balanced_text['tokens'].apply(util.big_words)\n",
    "test_balanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(test_balanced_style['big_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['huge_words'] = test_balanced_text['tokens'].apply(util.huge_words)\n",
    "test_balanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(test_balanced_style['huge_words'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3946f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_style['avg_word_size'] = train_imbalanced_text['tokens'].apply(util.average_word_length)\n",
    "train_imbalanced_style['small_words'] = train_imbalanced_text['tokens'].apply(util.small_words)\n",
    "train_imbalanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(train_imbalanced_style['small_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['big_words'] = train_imbalanced_text['tokens'].apply(util.big_words)\n",
    "train_imbalanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(train_imbalanced_style['big_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['huge_words'] = train_imbalanced_text['tokens'].apply(util.huge_words)\n",
    "train_imbalanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(train_imbalanced_style['huge_words'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a9c9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imbalanced_style['avg_word_size'] = test_imbalanced_text['tokens'].apply(util.average_word_length)\n",
    "test_imbalanced_style['small_words'] = test_imbalanced_text['tokens'].apply(util.small_words)\n",
    "test_imbalanced_style[['num_small_words', 'freq_small_words']] = pd.DataFrame(test_imbalanced_style['small_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['big_words'] = test_imbalanced_text['tokens'].apply(util.big_words)\n",
    "test_imbalanced_style[['num_big_words', 'freq_big_words']] = pd.DataFrame(test_imbalanced_style['big_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['huge_words'] = test_imbalanced_text['tokens'].apply(util.huge_words)\n",
    "test_imbalanced_style[['num_huge_words', 'freq_huge_words']] = pd.DataFrame(test_imbalanced_style['huge_words'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop(['small_words', 'big_words', 'huge_words'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d43e9e",
   "metadata": {},
   "source": [
    "## Sentence Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1924add",
   "metadata": {},
   "source": [
    "Another set of features could be related to various simple statistics about the size of a sentence, using both characters and words as a unit of size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1a7df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_balanced_style['avg_sent_size'] = train_balanced_text['sentences'].apply(util.average_sentence_length)\n",
    "train_balanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(train_balanced_style['avg_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['std_sent_size'] = train_balanced_text['sentences'].apply(util.std_sentence_length)\n",
    "train_balanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(train_balanced_style['std_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['min_sent_size'] = train_balanced_text['sentences'].apply(util.min_sentence_length)\n",
    "train_balanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(train_balanced_style['min_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style['max_sent_size'] = train_balanced_text['sentences'].apply(util.max_sentence_length)\n",
    "train_balanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(train_balanced_style['max_sent_size'].tolist(), index=train_balanced_style.index)\n",
    "train_balanced_style = train_balanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "523243dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_balanced_style['avg_sent_size'] = test_balanced_text['sentences'].apply(util.average_sentence_length)\n",
    "test_balanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(test_balanced_style['avg_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['std_sent_size'] = test_balanced_text['sentences'].apply(util.std_sentence_length)\n",
    "test_balanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(test_balanced_style['std_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['min_sent_size'] = test_balanced_text['sentences'].apply(util.min_sentence_length)\n",
    "test_balanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(test_balanced_style['min_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style['max_sent_size'] = test_balanced_text['sentences'].apply(util.max_sentence_length)\n",
    "test_balanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(test_balanced_style['max_sent_size'].tolist(), index=test_balanced_style.index)\n",
    "test_balanced_style = test_balanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a3ec859",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imbalanced_style['avg_sent_size'] = train_imbalanced_text['sentences'].apply(util.average_sentence_length)\n",
    "train_imbalanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(train_imbalanced_style['avg_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['std_sent_size'] = train_imbalanced_text['sentences'].apply(util.std_sentence_length)\n",
    "train_imbalanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(train_imbalanced_style['std_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['min_sent_size'] = train_imbalanced_text['sentences'].apply(util.min_sentence_length)\n",
    "train_imbalanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(train_imbalanced_style['min_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style['max_sent_size'] = train_imbalanced_text['sentences'].apply(util.max_sentence_length)\n",
    "train_imbalanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(train_imbalanced_style['max_sent_size'].tolist(), index=train_imbalanced_style.index)\n",
    "train_imbalanced_style = train_imbalanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07c6ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imbalanced_style['avg_sent_size'] = test_imbalanced_text['sentences'].apply(util.average_sentence_length)\n",
    "test_imbalanced_style[['avg_sentence_chars', 'avg_sentence_words']] = pd.DataFrame(test_imbalanced_style['avg_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['std_sent_size'] = test_imbalanced_text['sentences'].apply(util.std_sentence_length)\n",
    "test_imbalanced_style[['std_sentence_chars', 'std_sentence_words']] = pd.DataFrame(test_imbalanced_style['std_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['min_sent_size'] = test_imbalanced_text['sentences'].apply(util.min_sentence_length)\n",
    "test_imbalanced_style[['min_sentence_chars', 'min_sentence_words']] = pd.DataFrame(test_imbalanced_style['min_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style['max_sent_size'] = test_imbalanced_text['sentences'].apply(util.max_sentence_length)\n",
    "test_imbalanced_style[['max_sentence_chars', 'max_sentence_words']] = pd.DataFrame(test_imbalanced_style['max_sent_size'].tolist(), index=test_imbalanced_style.index)\n",
    "test_imbalanced_style = test_imbalanced_style.drop(['avg_sent_size', 'std_sent_size', 'min_sent_size', 'max_sent_size'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec311a",
   "metadata": {},
   "source": [
    "The calculated features look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7542b6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_newlines</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_upper_sentences</th>\n",
       "      <th>num_lower_sentences</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>num_small_words</th>\n",
       "      <th>freq_small_words</th>\n",
       "      <th>num_big_words</th>\n",
       "      <th>freq_big_words</th>\n",
       "      <th>num_huge_words</th>\n",
       "      <th>freq_huge_words</th>\n",
       "      <th>avg_sentence_chars</th>\n",
       "      <th>avg_sentence_words</th>\n",
       "      <th>std_sentence_chars</th>\n",
       "      <th>std_sentence_words</th>\n",
       "      <th>min_sentence_chars</th>\n",
       "      <th>min_sentence_words</th>\n",
       "      <th>max_sentence_chars</th>\n",
       "      <th>max_sentence_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2377</td>\n",
       "      <td>False</td>\n",
       "      <td>1884</td>\n",
       "      <td>36</td>\n",
       "      <td>264</td>\n",
       "      <td>161</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4.810606</td>\n",
       "      <td>111</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>59</td>\n",
       "      <td>0.223485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>169.545455</td>\n",
       "      <td>24.090909</td>\n",
       "      <td>193.375924</td>\n",
       "      <td>19.681346</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>754</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>592</td>\n",
       "      <td>True</td>\n",
       "      <td>1311</td>\n",
       "      <td>47</td>\n",
       "      <td>195</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5.169231</td>\n",
       "      <td>70</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>59</td>\n",
       "      <td>0.302564</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>144.333333</td>\n",
       "      <td>21.666667</td>\n",
       "      <td>103.887760</td>\n",
       "      <td>16.653328</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1049</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1087</td>\n",
       "      <td>True</td>\n",
       "      <td>1636</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>82</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5.371901</td>\n",
       "      <td>94</td>\n",
       "      <td>0.388430</td>\n",
       "      <td>80</td>\n",
       "      <td>0.330579</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>162.500000</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>35.268258</td>\n",
       "      <td>6.968501</td>\n",
       "      <td>117</td>\n",
       "      <td>17</td>\n",
       "      <td>217</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>725</td>\n",
       "      <td>False</td>\n",
       "      <td>784</td>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.617391</td>\n",
       "      <td>37</td>\n",
       "      <td>0.321739</td>\n",
       "      <td>43</td>\n",
       "      <td>0.373913</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>110.285714</td>\n",
       "      <td>16.428571</td>\n",
       "      <td>53.424752</td>\n",
       "      <td>7.798482</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>195</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  class  num_chars  num_newlines  num_words  num_unique_words  \\\n",
       "0  2377  False       1884            36        264               161   \n",
       "1   592   True       1311            47        195                60   \n",
       "2  1049  False         33             0          5                 5   \n",
       "3  1087   True       1636            18        242                82   \n",
       "4   725  False        784             2        115                81   \n",
       "\n",
       "   num_sentences  num_upper_sentences  num_lower_sentences  avg_word_size  \\\n",
       "0             11                    9                    2       4.810606   \n",
       "1              9                    9                    0       5.169231   \n",
       "2              1                    1                    0       5.600000   \n",
       "3             10                   10                    0       5.371901   \n",
       "4              7                    7                    0       5.617391   \n",
       "\n",
       "   num_small_words  freq_small_words  num_big_words  freq_big_words  \\\n",
       "0              111          0.420455             59        0.223485   \n",
       "1               70          0.358974             59        0.302564   \n",
       "2                2          0.400000              2        0.400000   \n",
       "3               94          0.388430             80        0.330579   \n",
       "4               37          0.321739             43        0.373913   \n",
       "\n",
       "   num_huge_words  freq_huge_words  avg_sentence_chars  avg_sentence_words  \\\n",
       "0               1         0.003788          169.545455           24.090909   \n",
       "1               0         0.000000          144.333333           21.666667   \n",
       "2               0         0.000000           33.000000            5.000000   \n",
       "3               0         0.000000          162.500000           24.200000   \n",
       "4               0         0.000000          110.285714           16.428571   \n",
       "\n",
       "   std_sentence_chars  std_sentence_words  min_sentence_chars  \\\n",
       "0          193.375924           19.681346                   5   \n",
       "1          103.887760           16.653328                   8   \n",
       "2            0.000000            0.000000                  33   \n",
       "3           35.268258            6.968501                 117   \n",
       "4           53.424752            7.798482                  30   \n",
       "\n",
       "   min_sentence_words  max_sentence_chars  max_sentence_words  \n",
       "0                   1                 754                  79  \n",
       "1                   1                 300                  48  \n",
       "2                   5                  33                   5  \n",
       "3                  17                 217                  36  \n",
       "4                   4                 195                  30  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_balanced_style.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
